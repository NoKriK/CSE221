\newpage
\subsection{Network}

\subsubsection{Round trip time}

%//NOTE: following is the guide line
%Evaluate for the TCP protocol. For each quantity, compare both remote and loopback interfaces. Comparing the remote and loopback results, what can you deduce about baseline network performance and the overhead of OS software? For both round trip time and bandwidth, how close to ideal hardware performance do you achieve? In describing your methodology for the remote case, either provide a machine description for the second machine (as above), or use two identical machines.
%//NOTE: above is the guide line

\paragraph{Methodology}
We developped a basic tcp server which accept connections and sends back the
data it receives.
The tcp client connect to the server and does one write and one read of a message of
the size of the Maximum Segment Size (MSS).
The measurement time does not include the time it takes for the socket and
connection operations.
The RTT we measured is exactly the time it takes for data to go through a round trip from the system to the remote host.
The operation is repeated 10,000 times and the result reported is the average.

For the ICMP measure, we averaged the resulted on 100 runs.
We used the following command :
\begin{verbatim}
ping -n -s 127.0.0.1 1400 100
\end{verbatim}

The second machine is described in the Appendix \ref{sec:app-remotehost}.

\paragraph{Predictions}
The packet will produce at least 1 context switch on each equipment it goes
through.
A context switch on Solaris cost around 100,000 cycles, this
is equivalent to 0.03 ms.
We measured the context switch on Linux and the cost is in the same order.
A vmexit or vmentry operation cost around 1000 cycles.
The cost of transfering 1500 bytes of data at 100Mb/s is about 0.12ms.

The cost of remote request will be high as the packet will travel through :
\begin{enumerate}
\item The Solaris OS, producing a context switch to kernel land,
\item The Xen hypervisor through an hypercall, producing a context switch in the
hypervisor,
The Xen hypervisor also does Network Address Translation (NAT), which may cost
around 10,000 cycles,
\item Two or more switches, they are embedding real time operating systems,
but they still increase latency,
\item The ESXi hypervisor through an interrupt on the network card, which
produce a context switch to the ESXi kernel,
\item The linux guest OS,
\end{enumerate}

As the ICMP requests are handled at the kernel level on the Linux side this will
avoid two context switches.
The TCP protocol is statefull and the packet will be checked against the state
tables in the solaris and linux virtual machine but also on the Xen hypervisor
(router/NAT) which could cause an higher cost.
We don't except to see a real difference between the ICMP and the TCP request
due to the number of unpredictable equipment the packet need to traverse.

For the remote host, the minimal cost will be around 8 context switches (4 on
each way).
This is about 0.24 ms + the cost of the data transfer which is twice 0.12ms so a total of
about 0.48ms.

For the loopback interface we can estimate the cost to two context switchs for
the ICMP packets and four context switchs for the TCP packets.
The memory operation cost is negligable.
This would respectivly make about 0.06ms and 0.12ms.




\paragraph{Results}
% remote tcp : 4146300.853200 cycles 1.26ms
% local tcp : 214142.5357 cycles 65 us

\begin{table}[h]
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
\hline
Operation 		& Hardware cost 	& Software cost 	& Prediction 		& Measured \\ \hline
Local ICMP rtt		&			&			&			& 		\\ \hline
Remote ICMP rtt		&			&			&			& 		\\ \hline
Local TCP rtt		&		& 	&		& 65 $\mu$s\\ \hline
Remote TCP rtt		&			&			& & 1.26 ms	\\ \hline


\end{tabular}
\end{center}
\caption{Round trip time\label{tab:rtt}}
\end{table}

As seen in table \ref{tab:rtt}
\paragraph{Success of Methodology}

 Compare with the time to perform a ping (ICMP requests are handled at kernel level).






\subsubsection{Peak bandwidth}
\paragraph{Methodology}
We measured the time elapsed when transfering 2GB of data.
The remote tcp server just drops the packet it receives.
The client send as much data as he can by packet of the size of the MSS.

We ran the test 1000 times for the loopback, 5 times for the remote bandwidth
and reported the averaged result.

\paragraph{Predictions}
The slowest network switch is switching at the theoritical speed of 100Mb/s.
We except the switch to be the bottleneck and to be able to handle only 90\% of
the theoritical 100Mb/s so 11.25MB/s.
The overhead of TCP/IP is about 1/

For the loopback device, the bottleneck will be the bandwidth of the RAM.
The data needs to be copied in the kernel buffers on client request, read from
the buffer kernel and written to the userland buffer on the server side.
This gives us two read and two write operation and a theoritical bandwith of
about 2161MB/s.
Thus we count one mode switch overhead (1000 cycles) per read/write which gives
us an overhead of 23 ms per GB transferred (about 760,000 syscalls).
On a single core we would have accounted for two mode switch but the two virtual
cores may help the data processing parallalization.
The resulting estimated bandwidth is 1455 MB/s.

\paragraph{Results}
% loopbach = 5971829937 cycles or 1132MB/s
% remote = 640049329512 cycles or 10.56MB/s
\begin{table}[h]
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
\hline
Operation & Hardware performance & Software penality & Prediction & Measured \\
\hline
Local 	&  2161 MB/s	& 23 ms/GB	&  1455 MB/s &  1132MB/s \\
\hline
Remote 	&  11.25 MB/s	& 0 &  11.25MB/s	&  10.56MB/s \\
\hline

\end{tabular}
\end{center}
\caption{Peak bandwidth\label{tab:peak-bandwidth}}
\end{table}

As seen in table \ref{tab:peak-bandwidth}
\paragraph{Success of Methodology}
To improve the result on the loopback interface, one solution would have been to
increase the buffer size We may have written more data on the socket but it doesn't sound like the
bottleneck ?


\subsubsection{Connection overhead}
\paragraph{Methodology}
We basicly connected to the remote server and to the looppack device 100,000 times.

\paragraph{Predictions}
Due to the topology of the network and the number of equipment to traverse we
except to connection overhead to be about 6-10 times higher for the remote server.
\paragraph{Results}
%Connection avg time : 32667528
%Close avg time : 45015
\begin{table} [h]
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
\hline
Operation & Hardware cost & Software cost & Prediction & Measured \\ \hline
Local setup 	&  cycles		& 10			&  cycles	&  cycles \\ \hline
Remote setup	&  cycles		& 10			&  cycles	&  cycles \\ \hline
Local tear-down 	&  cycles		& 10			&  cycles	&  cycles \\ \hline
Remote tear-down	&  cycles		& 10			&  cycles	&  cycles \\ \hline



\end{tabular}
\end{center}
\caption{Connection overhead\label{tab:connection-overhead}}
\end{table}

As seen in table \ref{tab:connection-overhead}

\paragraph{Success of Methodology}
