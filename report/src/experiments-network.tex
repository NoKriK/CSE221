\newpage
\subsection{Network}

\subsubsection{Round trip time}

%//NOTE: following is the guide line
%Evaluate for the TCP protocol. For each quantity, compare both remote and loopback interfaces. Comparing the remote and loopback results, what can you deduce about baseline network performance and the overhead of OS software? For both round trip time and bandwidth, how close to ideal hardware performance do you achieve? In describing your methodology for the remote case, either provide a machine description for the second machine (as above), or use two identical machines.
%//NOTE: above is the guide line

\paragraph{Methodology}
We developped a basic tcp server which accept connections and sends back the
data it receives.
The tcp client connect to the server and does one write and one read of a message of
the size of the Maximum Segment Size (MSS).
The measurement time does not include the time it takes for the socket and
connection operations.
The RTT we measured is exactly the time it takes for data to go through a round trip from the system to the remote host.
The operation is repeated 10,000 times and the result reported is the average.

For the ICMP measure, we averaged the resulted on 100 runs.
We used the following command :
\begin{verbatim}
ping -n -s 127.0.0.1 1400 100
\end{verbatim}

The second machine is described in the Appendix \ref{sec:app-remotehost}.

\paragraph{Predictions}
The packet will produce at least 1 context switch on each equipment it goes
through.
A context switch on Solaris cost around 100,000 cycles, this
is equivalent to 0.03 ms.
We measured the context switch on Linux and the cost is in the same order.
A vmexit or vmentry operation cost around 1000 cycles.
The cost of transfering 1500 bytes of data at 100Mb/s is about 0.12ms.

The cost of remote request will be high as the packet will travel through :
\begin{enumerate}
\item The Solaris OS, producing a context switch to kernel land,
\item The Xen hypervisor through an hypercall, producing a context switch in the
hypervisor,
The Xen hypervisor also does Network Address Translation (NAT), which may cost
around 10,000 cycles,
\item Two or more switches, they are embedding real time operating systems,
but they still increase latency,
\item The ESXi hypervisor through an interrupt on the network card, which
produce a context switch to the ESXi kernel,
\item The linux guest OS,
\end{enumerate}

As the ICMP requests are handled at the kernel level on the Linux side this will
avoid two context switches.
The TCP protocol is statefull and the packet will be checked against the state
tables in the solaris and linux virtual machine but also on the Xen hypervisor
(router/NAT) which could cause an higher cost.
We don't except to see a real difference between the ICMP and the TCP request
due to the number of unpredictable equipment the packet need to traverse.

For the remote host, the minimal cost will be around 8 context switches (4 on
each way).
We also need to account for the processing time of the switches.
We can except them to be more efficient, we will count with 0.02ms per switch traversal.
This is about 0.24 ms + 0.08ms (switchs) + the cost of the data transfer which is twice 0.12ms so a total of
about 0.48ms.

For the loopback interface we can estimate the cost to two context switchs for
the ICMP packets and four context switchs for the TCP packets.
The memory operation cost is negligable.
This would respectivly make about 0.06ms and 0.12ms.




\paragraph{Results}
% remote tcp : 4146300.853200 cycles 1.26ms
% local tcp : 214142.5357 cycles 65 us
% remote icmp : 1.253 ms
% local icmp : 0.063ms

\begin{table}[h]
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
\hline
Operation 	& Hardware cost 	& Software cost 	& Prediction 		& Measured \\ \hline
Local ICMP rtt	& 0 ms	& 0.06 ms & 0.06 ms  & 0.063 ms	\\ \hline
Local TCP rtt	& 0 ms	& 0.012 ms & 0.012 ms	& 0.065 ms\\ \hline
Remote ICMP rtt	& 0.12 ms	& 0.32 ms & 0.56ms  & 1.253 ms\\ \hline
Remote TCP rtt	& 0.12 ms	& 0.32 ms & 0.56ms  & 1.26 ms	\\ \hline


\end{tabular}
\end{center}
\caption{Round trip time\label{tab:rtt}}
\end{table}

As seen in table \ref{tab:rtt}, our estimation for the local latency seems
accurate.
The TCP RTT is not way higher.
We have much overhead due to the number of equipment we are going through.

\paragraph{Success of Methodology}
We wish we had time to test with the hypervisor to see how the latency was
increasing.



\subsubsection{Peak bandwidth}
\paragraph{Methodology}
We measured the time elapsed when transfering 2GB of data.
The remote tcp server just drops the packet it receives.
The client send as much data as he can by packet of the size of the MSS.

We ran the test 1000 times for the loopback, 5 times for the remote bandwidth
and reported the averaged result.

\paragraph{Predictions}
The slowest network switch is switching at the theoritical speed of 100Mb/s.
We except the switch to be the bottleneck and to be able to handle only 90\% of
the theoritical 100Mb/s so 11.25MB/s.
We can estimate the overhead of TCP/IP about 2\% wich gives us about 11MB/s.

For the loopback device, the bottleneck will be the bandwidth of the RAM.
The data needs to be copied in the kernel buffers on client request, read from
the buffer kernel and written to the userland buffer on the server side.
This gives us two read and two write operation and a theoritical bandwith of
about 2161MB/s.
Thus we count one mode switch overhead (1000 cycles) per read/write which gives
us an overhead of 23 ms per GB transferred (about 760,000 syscalls).
On a single core we would have accounted for two mode switch but the two virtual
cores may help the data processing parallalization.
The resulting estimated bandwidth is 1455 MB/s.

\paragraph{Results}
% loopbach = 5971829937 cycles or 1132MB/s
% remote = 640049329512 cycles or 10.56MB/s
\begin{table}[h]
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
\hline
Operation & Hardware performance & Software penality & Prediction & Measured \\
\hline
Local 	&  2161 MB/s	& 23 ms/GB	&  1455 MB/s &  1132MB/s \\
\hline
Remote 	&  11.25 MB/s	& 0 &  11.25MB/s	&  10.56MB/s \\
\hline

\end{tabular}
\end{center}
\caption{Peak bandwidth\label{tab:peak-bandwidth}}
\end{table}

As seen in table \ref{tab:peak-bandwidth}, our estimation for the loopback are
accurate.
We probably underestimated the overhead of the mode switches.
We also probably underestimated the overhead of TCP.

\paragraph{Success of Methodology}
To improve the result on the loopback interface, one solution would have been to
increase the buffer size.
Overall our methodology seems to give accurate results.

\subsubsection{Connection overhead}
\paragraph{Methodology}
We basicly connected to the remote server and to the loopback device 100,000 times.
We read the clock before and after the \emph{connnect} system call and did the
same thing for the \emph{close} system call.

\paragraph{Predictions}
Due to the topology of the network and the number of equipment to traverse we
except to connection overhead to be about 20 times higher for the remote server.
We except the connection setup cost to be twice the rttp, so about 2.5ms.

The \emph{close} is asynchronous so we doesn't except any difference between
remote and local close.
The cost should be near an expensive call, around 10,000 cycles (0.003 ms).

\paragraph{Results}
%Connection avg time : 32667528
%Close avg time : 45015
%Local
%Connection avg time : 244397
%Close avg time : 55428

\begin{table} [h]
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
\hline
Operation & Hardware cost & Software cost & Prediction & Measured \\ \hline
Local setup 	&  0.12 ms	& 2.48 ms	&  2.5 ms & 0.074 ms \\ \hline
Remote setup	&  0.12 ms	& 2.48 ms	&  2.5 ms&  9.899 ms \\ \hline
Local tear-down 	&  0	& 0.003 ms	&  0.003 ms&  0.017 ms \\ \hline
Remote tear-down	&  0	& 0.003 ms	&  0.003 ms&  0.013 ms \\ \hline



\end{tabular}
\end{center}
\caption{Connection overhead\label{tab:connection-overhead}}
\end{table}

As seen in table \ref{tab:connection-overhead}, we were optimist on the cost of
a tcp connection setup.

\paragraph{Success of Methodology}
Our results seems accurate.
