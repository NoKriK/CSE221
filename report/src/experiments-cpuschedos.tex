\subsection{CPU, Scheduling, and OS Services}

\subsubsection{Measurement overhead}

\paragraph{Methodology}
As we are microbenchmarking, the overhead of the measurement tools that we
choosed may influence our results. Thus we need to know the cost of reading
the clock and since we are using loops for averaging the results of the tests, we
also need to know the overhead caused by these loops.\\
We are using the \emph{rdtsc} assembly instruction to do the measurement.
The \emph{rdtsc} instruction allows us to read the \emph{Time Stamp Counter}
which is a 64 bit counter containing the number of cycles elapsed since his
reset.\\
In terms of implementation, the instruction is integrated in the code using
inline assembly and is called using a function which is inlined to avoid the
overhead of the procedure call and the stack frame creation.
To measure the cost of the reading clock operation, we measured the result between
two calls and repeated the operation 100,000 times.\\
The loop overhead has been measured with 100,000 iterations.

\paragraph{Predictions}
The operations between the two rdtsc call are basicly a copy from the result
which are stored in the eax and edx register to the main memory, a binary shift
and a binary OR on the two variables.
As the cache may be hited, it's basicly two cache write, around 10 cycles, and
the cost of the second rdtsc call.\\

As all optimization are turned off, the cost of a loop iteration is basicly a
load from memory, an incrementation, a store to memory, a comparaison and a
jump.
The cache will probably be hited, thus the cost of memory operations will be low.
The total cost may be around 15 cycles if we count 10 cycles for the memory
latency and 1 cycle for operation such as incremention, comparaison and
jump.\\

There is no software overhead in any of these operations.

\paragraph{Results}
\begin{table}[h]
\begin{center}
\begin{tabular}{| l | l | l | l | l |} \hline
Operation 			& Hardware cost 	& Software cost 	& Prediction	& Measured \\ \hline
Reading the clock 	& 25 cycles		& 0 cycles			& 25 cycles 	& 58.555 cycles \\ \hline
Loop 				& 15 cycles 		& 0 cycles 		& 15 cycles 	& 12.487 cycles \\ \hline
\end{tabular}
\end{center}
\caption{Measurement overhead\label{tab:measurement-overhead}}

\end{table}

As seen in table \ref{tab:measurement-overhead}, our estimates are too low for the reading time compared to the actual result. The reason of that is the cost of the memory write operation is
higher than excepted. It seems that we fill the cache latency due to the dependency of the data avoiding the effective use of the pipelining.
This hypothesis on the memory write operation cost is due to the fact that on long run,
the result is lowered to aproximatly 30 cycles after 50,000 iterations.
We choosed to report the average of a run which did not show this reduction in
the cost because it seems more accurate (i.e. we will call rdtsc once for each
loop).\\

Our estimates are in the good range for the loop.

\paragraph{Success of Methodology}
Our result for the reading time are varying but we have an average result. As
the overhead will be accounted only once for many iterations, this will not
cause more problem.\\

The result for the loop overhead seems accurate.

\subsubsection{Procedure call overhead}
\paragraph{Methodology}
We defined seven functions with different number of arguments to test the
cost of a procedure call and the overhead of an argument.
Each procedure call is made 100,000 time in a loop and the total is measured.
The cost per loop cycle is then calculated and the overhead of the loop is
removed.

\paragraph{Predictions}
On x86-64 a procedure call is composed off :
\begin{enumerate}
\item Moving the arguments into specific registers (for the 7 first arguments)
\item Calling the procedure, that is jumping to the beginning of the procedure
\item Creating a new stack frame, that is saving the previous stack pointer and
moving the stack pointer into the begin stack pointer
\item Destroying the stack frame
\item Restoring the instruction pointer
\end{enumerate}

The latency of the memory operations may be hidden due to pipelining.
The procedure call should normally be predicted by the branch prediction part of
the processor avoiding stall of the processor.
The architecture of the Intel Sandy Bridge is very complex, making prediction is
really difficult.
We can guess that the procedure call will take about 10 cycle and that each
argument will cause a cost of one cycle due to the new register access.

\paragraph{Results}
\begin{table}[h]

\begin{center}
\begin{tabular}{| l | l | l | l | l |}
\hline
\# of args 	& Hardware cost 	& Software cost & Prediction & Measured \\ \hline
0 & 2 cycles 	& 8 cycles  		& 10 cycles 	& 14.471 cycles \\ \hline
1 & 2 cycles 	& 9 cycles  		& 11 cycles 	& 14.471 cycles \\ \hline
2 & 2 cycles 	& 10 cycles  		& 12 cycles 	& 14.455 cycles \\ \hline
3 & 2 cycles 	& 11 cycles 		& 13 cycles 	& 14.462 cycles \\ \hline
4 & 2 cycles 	& 12 cycles 		& 14 cycles 	& 14.461 cycles \\ \hline
5 & 2 cycles 	& 13 cycles 		& 15 cycles 	& 14.461 cycles \\ \hline
6 & 2 cycles 	& 14 cycles 		& 16 cycles 	& 14.456 cycles \\ \hline
7 & 2 cycles 	& 15 cycles 		& 17 cycles 	& 16.518 cycles \\ \hline
\end{tabular} \\
\end{center}
\caption{Procedure calls\label{tab:procCall}}
\end{table}
\begin{figure}[h]

\begin{center}
\includegraphics[scale=0.8]{procCallImage}
\end{center}
\caption{Procedure calls\label{fig:procCallf}}
\end{figure}




As what we can see in figure \ref{fig:procCallf} and table \ref{tab:procCall}, The results are low as predicted. Apparently the cost of accessing one more argument is too low to be detected. Only when the function take 7 aguments, which cannot all fit into the registers. The 7th argument is put into the stack and the cost for doing that turns out to be about 2 cycles.

\paragraph{Success of Methodology}
The way we used for testing procedure call is quite successful. At first, we tried to use a function sums up the parameters it takes in. As a result, the running time for procedures with different number of arguments are different. However,that is mainly because of the fact more add operations are preformed in that case. After we changed the functions to be a int type always return 1, we got result shown in figure \ref{fig:procCallf}. At first we were really confused about it, later we realized that not all of the arguments are put on to the stack on x86-64 CPUs.







\subsubsection{System call overhead}
\paragraph{Methodology}
We choosed the getpid() system call to test the overhead for a minimal system
call as it's supposed to be the least expensive system call.
We then ran it 10,000 times to get an averaged result.

\paragraph{Predictions}
System call are expected to be more expensive than simple procedure call as they
produce a context switch.
The state of the process need to be saved and a kernel thread must be launched.
The way back need also to be done.
We except something like 400 cycles for each context switch, so a total of about 800 cycles.

\paragraph{Results}
\begin{table} [h]
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
\hline
Operation 				& Hardware cost & Software cost & Prediction & Measured \\
\hline
Context switch (1st run) & 200 cycles & 600 cycles & 800 cycles & 5000 cycles\\
\hline
Context switch (average) & 200 cycles & 600 cycles & 800 cycles & 750 cycles\\
\hline
\end{tabular}
\end{center}

\caption{System calls\label {tab:sysCall}}
\end{table}


As what we can see in table \ref{tab:sysCall}, The first call is always slow, due to the need to read kernel code and
kernel memory which are not in cache. As we discussed ealier, the fact we are using virtual machine is giving us a worse performance on TLB misses. The first system call or procedure call will always experience a TLB miss.
The following calls are going faster probably due to branch prediction and
cache.

\paragraph{Success of Methodology}
We can see that system calls are much expensiver than procedure calls as excepted. Here is the reasons, first of all, system call by itself is a normal procedure call, it need to do everything a procedure call need to do. Secondly, the system call need more time for loading function and switching to the kernel mode. The way we used for measuring system and procedure calls are really successful.

\subsubsection{Task creation time}
\paragraph{Methodology}
We measured the time to create process and thread seperately. After that, we compared them to each other to verify our measurement. When a new process is created, the page tables and stack frame are copied. While for creating a kernel thread, less information are required to get copied because of the fact that a lot of the resource in thread are shared with the main thread.
We used the system call fork() for creating a new process and record the number of cycles from before the fork() is called to the point waitid() get back from the new process. And for the newly created process, we called \_exit() to immediated exit after creation.
Similarly, we used the solaris implementation, which is thr\_create() for cloning thread. At first we used pthread\_create() fuction for thread creation operation. We replaced it with thr\_create() later because it is not meant for Solaris system, which is not as accurate when we are doing the measurement of the whole Solaris OS. Right after the creation of the thread, we used thr\_join() to wait for the termination of itself.
At the same time, we measured the time for the program to create process and threads for the first time only as well, for loading the functions we used in this measurement into the cache.
The creation overhead has been measured with 100,000 iterations.



\paragraph{Predictions}

As mentioned earlier, the thread has less data to copy than process do. Meanwhile, the process creation is using system call instead of function call, which envolves two context switching time. We predict that process creation is about 10000 cycles taking longer than thread creation 8000 cycles.






\paragraph{Results}
\begin{table}[h]
\begin{center}
\begin{tabular}{| l | l | l | l | l | l |}
\hline
Operation & H/W cost & S/W cost & Prediction & Measured average & Measured first creation \\
\hline
fork() process 		& 20 cycles & 10000 cycles 	& 10000 cycles& 1198519.65 cycles & 2257852 cycles\\ \hline
thr\_create() thread 	& 20 cycles & 8000 cycles		& 8000 cycles &  157433.27 cycles & 2273593 cycles\\ \hline
\end{tabular}
\end{center}
\caption{Task creation time\label{tab:task-creation}}

\end{table}

As seen in table \ref{tab:task-creation}, the measurement result mostly fits our prediction besides the fact that the number of cycles are way bigger than what we expected.
\paragraph{Success of Methodology}
For testing the overhead of creating a new process, we simply called fork(). It turned out that it takes about 750,000 clock cycles for creating it without caching and less than 500,000 with caching.While for the overhead of creating a new thread, we simply called pthread\_creat(). It turned out that it takes about 2000,000 clock cycles for creating it without caching and less than 80,000 with caching.
The fact thread creation without caching is taking way longer time than the one for process creation doesn't really make sense to us right now, and we will look into that later as the project goes. But considering that thread creation does't envolve memory alloction, the average case for thread creation is generally faster then the one for process creation.


\subsubsection{Context switch time}
\paragraph{Methodology}

We measured the context switching time for both processes and threads. We used pipes in order to measure them. However, becasue of the fact that pipes have quite a big overhead by itself. We measured it seperately at the same time. Pipe would force a context switch into kernel mode when they are reading or writing. Similar to the previous task creation measurement, we used fork() and thr\_create() for creating new process and threads. The main/parent process waits to read from pipe where the child process or thread is writing to. 





\paragraph{Predictions}
Similar to task creation time, we would expect to see a smaller amount of time for thread context switching than process context switching. Based on the tasked creation time we can see in table \ref{tab:task-creation}, our prediction for process context switching is 2,000,000 clock cycles and for process context switching is 1,500,000 clock cycles. Meanwhile, our prediction for the pipe overhead is 30000 clock cycles.



\paragraph{Results}
\begin{table}
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
\hline

Operation 				& Hardware cost 	& Software cost 	& Prediction 	& Measured \\ \hline

process context switch 	& 				&				&2000000 cycles	& 2010805.721 cycles \\
\hline
thread context switch 	& 				&				&1500000 cycles	& 1895295.586 cycles \\ 
\hline
pipe overhead			& 				&				&30000 cycles	& 45275.283 cycles \\ 
\hline
\hline
\end{tabular}
\end{center}

\caption{Context switch time\label{tab:context-switch-time}}
\end{table}
As what we can see in table \ref{tab:context-switch-time}, same as what we expected, the time it takes for context switching between threads is a lot shorter than the one for processes.

\paragraph{Success of Methodology}
We measured the context switching overhead for both processes and threads. The overhead of such context switching is going to be the time start from the creation of the child to the time execution gets back to the parent process/thread excluding the time for child to execute. We only tested the context switching within 2 different processes. Due to the lack of time, we were not measuring the context switching time among more than 2 processes. However, this is something good to measure in a real situation because it makes more sense to execute the program using the optimized number of threads in order to get ebst performance.



